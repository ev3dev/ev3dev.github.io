---

author: [ "@bmegli" ] 
programming_language: [ "C/C++/C#" ] 

#youtube_video_id: G6uVg34VzHw # The video ID of the YouTube video to be displayed with this post

#project_homepage_url: "http://example.com/my-super-cool-project" # Homepage for this project
source_code_url: "https://github.com/bmegli/ev3dev-mapping" # Provide a link to your code
#building_instructions_url: "" # how to build the model out of LEGO (*not* how to build the source code)

excerpt: "A real-time 3D mapping project." # Yet another ev3dev project.
---

## Overview

A 2D/3D mapping/scanning project consisting off:

-  robot side code
-  cross-platform visualization

[Robot side code] is really a bunch of super-simple programs which I call modules.
Each of those programs works perfectly well from the command line without the rest of the project.
The sole exception is [ev3control] that takes care off enabling, disabling and monitoring other modules. 
[ev3control] is also super-insecure at this stage (can run arbitrary code on the robot!).

[Cross-platform visualization] is a [Unity] project that can plot the readings in real-time and/or control the robot.
Like other projects written in Unity it works on more than 10 [platforms], in theory at least. 
Unity has also a nice abstraction for input so robots can be controlled with keyboards/pads/mice/wheels/mobiles and so on.

To make the project more spicy the budget (including EV3) was constrained to 400$ (360 â‚¬) and hardware to EV3 with lowest specs of ev3dev platforms which results in DIY solutions.

The meta-repository of the project is [ev3dev-mapping].

## Examples

The examples are X3D models. You can rotate, move and zoom with the mouse.

{% include x3d-model.html caption="Example Phase II raw scan" source="\images\projects\2016-08-07-Mapping\csi_body.x3d" %}

{% include x3d-model.html caption="Example Phase II result + MeshLab surface reconstruction" source="\images\projects\2016-08-07-Mapping\software_engineer.x3d" %}

## Phase I

Goals:

1. Hardware selection
-  CruizCore XG1300L gyroscope for low bias drift and great accuracy
-  XV11 Lidar for unmatched... price 
2. Drivers
-  CruizCore driver was easy with wonderful @dlech sensor driver framework ready
-  XV11 Lidar was moderate with all the great info from [xv11hacking]
3. Correct geometry
-  nobody (on the internet) seemed to treat XV11 [rotational geometry] correctly
4. LIDAR standard deviation
-  evaluation with respect to distance
5. Experimental 2D mapping
-  experimental code to learn and throw away

The Phase I is finished with results:

-   [Using the XV11 Lidar tutorial]
-   [xv11lidar] C/C++ library
-   [mi-xg1300l] CruizCore XG1300L driver 
-   [ev3dev-mapping-abandoned] repository

The Phase I robot in action:

-   [How to interface XV11 LIDAR to EV3 using ev3dev]
-   [EV3 Gyro vs CruizCore XG1300L vs Odometry - Position Estimation]

{% include screenshot.html source="\images\projects\2016-08-07-Mapping\mapping-phase0-up-down.jpg" caption="Hardware used for testing in Phase I" %}

## Phase II

Goals:

1. Cross-platform visualisation/computation/control
-   [Unity] chosen for amazing [platforms] support
-   C# support
-   built in rigid bodies, navigation, easy custom shaders...
2. Modular architecture design
-   modules as simple command line programs in any language
-   supervisor on top that does 'magic'
-   UI components counterpart of modules 
3. 3D mapping with lidars in arbitrary position/rotation
-   XY and XZ plane support with any position, generalizing is easy
4. Mapping in movement
-   compromise like move/stop/map is unacceptable!
-   moving/rotating slowly for better accuracy is ok
-   per reading interpolation for position/heading
5. Support for millions of points on average laptop
-   lidar spits around 100k points a minute!
6. Network communication recording/replaying
-   for running same data with different algorithms/parameters
-   for re-running old data with new features
-   for reprocessing the data on more powerful machine if needed
7. Data export to widely-accepted file format
-   [ply](https://en.wikipedia.org/wiki/PLY_(file_format)) chosen for ease of implementation
8. Multiple lidars support at the same time
-   just use multiple Laser components
9. Multiple robots support at the same time
-   no problem but drive (keyboard, pad) scripted for single robot now
10. Extensibility for different hardware
-   this needs writing own modules/components

Phase II is functionally finished now with results:

-    [ev3dev-mapping-ui] Unity project repository
-    [ev3dev-mapping-modules] robot modules repository
-    [ev3dev-mapping-ui-udp] recorded UDP communication from some mappings
-    [ev3dev-mapping-results] with screenshots/3D models
 
There are some things that need to be done though:

-    ev3control & Control are rewritten for TCP/IP, UDP is working but not suitable for that
-    Unity component model is reworked from flat structure to hierarchical structure (robot with child subsystems instead of with all components)
-    common properties are moved to separate components (e.g. Network Configuration component)
-    component dependencies are reworked (e.g. Laser, LaserRenderer, LaserUI components instead of Laser with nested renderer, and ui prefabs)
-    UI needs some work but I am rather leaning towards built-in console


## Architecture



## Modules & Components



 

## Building Instructions

For electrical/mechanical XV11 Lidar interfacing see [Using the XV11 Lidar tutorial]

{% include screenshot.html source="\images\projects\2016-08-07-Mapping\mapping-phase1-3x2.jpg" caption="Hardware used for testing in Phase II" %}

One XY-plane lidar is enough for 3D scanning. The XZ-plane lidar is for orientation 
and Phase III project.

## Other

It's perfectly ok to use only a subset of features:

- 2 engines are enough to use Drive component with [ev3drive](https://github.com/bmegli/ev3dev-mapping-modules/tree/master/ev3drive) module (UDP control)
- XV11 Lidar is enough to use Laser component with [ev3laser](https://github.com/bmegli/ev3dev-mapping-modules/tree/master/ev3laser) module (readings in local reference frame)
- 2 engines + CruizCore 1300L gyroscope allows to use Odometry component with [ev3odometry](https://github.com/bmegli/ev3dev-mapping-modules/tree/master/ev3odometry) module
- all of above makes mapping possible (precisely Odometry + Laser + PositionHistory components do that)


[kramdown](http://kramdown.gettalong.org/syntax.html) markdown engine.


## Phase III

Yes... but Phase II was completely written in the second half of July when my kids were on holidays with grandparents. I do feel burnt out now.

## References

[ev3dev-mapping]: https://github.com/bmegli/ev3dev-mapping
[Robot side code]: https://github.com/bmegli/ev3dev-mapping-modules
[ev3dev-mapping-modules]: https://github.com/bmegli/ev3dev-mapping-modules
[Cross-platform visualization]: https://github.com/bmegli/ev3dev-mapping-ui
[ev3dev-mapping-ui]: https://github.com/bmegli/ev3dev-mapping-ui
[ev3dev-mapping-ui-udp]: https://github.com/bmegli/ev3dev-mapping-ui-udp
[ev3dev-mapping-results]: https://github.com/bmegli/ev3dev-mapping-results

[ev3control]: https://github.com/bmegli/ev3dev-mapping-modules/tree/master/ev3control

[Unity]: https://unity3d.com/
[platforms]: https://unity3d.com/unity/multiplatform
[xv11hacking]: https://xv11hacking.wikispaces.com/LIDAR+Sensor
[rotational geometry]: /docs/tutorials/using-xv11-lidar/#lidar-rotational-geometry
[Using the XV11 Lidar tutorial]: /docs/tutorials/using-xv11-lidar/
[xv11lidar]: https://github.com/bmegli/xv11lidar C/C++ library
[mi-xg1300l]: http://www.ev3dev.org/docs/sensors/microinfinity-digital-gyroscope-and-accelerometer/ 
[ev3dev-mapping-abandoned]: https://github.com/bmegli/ev3dev-mapping-abandoned

[How to interface XV11 LIDAR to EV3 using ev3dev]: https://www.youtube.com/watch?v=G6uVg34VzHw
[EV3 Gyro vs CruizCore XG1300L vs Odometry - Position Estimation]: https://www.youtube.com/watch?v=vzND_ISdhEs